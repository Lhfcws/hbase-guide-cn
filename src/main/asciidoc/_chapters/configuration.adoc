////
/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
////

[[configuration]]
= Apache HBase Configuration
:doctype: book
:numbered:
:toc: left
:icons: font
:experimental:

本章提供了基于 <<getting_started>> 章节的 Apache HBase 配置拓展.
请认真阅读本章, 尤其是 <<basic.prerequisites,Basic Prerequisites>> ，从而保证您的HBase集群测试与部署正常，并防止数据丢失.

== Configuration Files
Apache HBase 使用与 Apache Hadoop 一致的配置体系.
所有的配置文件都放在 _conf/_ 目录下, 该目录需要做到多节点间保持同步一致.

.HBase Configuration File Descriptions
_backup-masters_::
  默认不存在该文件.
  纯文本文件，列出哪些节点需要启动backup mater进程，每行一个host。

_hadoop-metrics2-hbase.properties_::
  用以连接 HBase Hadoop's Metrics2
  See the link:http://wiki.apache.org/hadoop/HADOOP-6728-MetricsV2[Hadoop Wiki entry] for more information on Metrics2.
  默认仅包含注释示例.

_hbase-env.cmd_ and _hbase-env.sh_::
  环境配置脚本 for HBase，包含java位置、jvm选项、环境变量等。
  默认包含众多注释示例作为指导.

_hbase-policy.xml_::
  当客户端请求时，RPC servers所使用的权限策略配置文件。
  当 HBase <<security,security>> 被启用时才会使用该配置.

_hbase-site.xml_::
  HBase主配置文件.
  在该配置中显式指定的配置会覆盖默认配置。
  默认配置参见（请勿修改） _docs/hbase-default.xml_.
  在 HBase Web UI 的 [label]#HBase Configuration# ，您可以看到所有集群当前生效的配置（包含默认的和重载的）。

_log4j.properties_::
  HBase的`log4j`日志配置。

_regionservers_::
  纯文本文件，列出需要启动regionServer进程的节点，每行一个host。
  默认是一个只包含一行 `localhost` 的文件。
  该文件应当包含一组hostnames或IP地址（每行一个）。如果集群里的每个节点都以`localhost`接口来运行`RegionServer`的话，则只需像默认一样包含`localhost`即可。

.验证 XML 有效性
[TIP]
====
您最好使用可以检查XML语法的一些IDE或编辑器。
您也可以`xmllint` 来检查XML的语法。
默认情况下, `xmllint` 会将xml格式化输出到标准输出流。
如果仅用来检查语法并只希望输出错误信息, 请使用命令 `xmllint -noout filename.xml`。
====
.确保集群内配置保持同步
[WARNING]
====
在分布式模式下运行HBase时, 若您编辑过HBase配置, 请确保您已将最新配置拷贝（`rsync`, `scp`, 或其他一些配置管理分发工具等）到所有节点的 _conf/_ 目录下。
HBase不会帮你做分布式节点配置同步的事情。
除了动态配置，其他绝大部分配置都属于静态配置，都需要HBase集群重启后生效。
====

[[basic.prerequisites]]
== 基础环境要求 Basic Prerequisites

本章节列出了需要的一些前提服务和系统配置要求。（译者注：总之表格里只需要关注 yes 项即可，其他建议都当做 no 谨慎看待，防止挖坑。1.2+确认支持JDK8）

[[java]]
.Java
[cols="1,1,1,4", options="header"]
|===
|HBase Version
|JDK 6
|JDK 7
|JDK 8

|1.2
|link:http://search-hadoop.com/m/DHED4Zlz0R1[Not Supported]
|yes
|yes

|1.1
|link:http://search-hadoop.com/m/DHED4Zlz0R1[Not Supported]
|yes
|Running with JDK 8 will work but is not well tested.

|1.0
|link:http://search-hadoop.com/m/DHED4Zlz0R1[Not Supported]
|yes
|Running with JDK 8 will work but is not well tested.

|0.98
|yes
|yes
|Running with JDK 8 works but is not well tested. Building with JDK 8 would require removal of the
deprecated `remove()` method of the `PoolMap` class and is under consideration. See
link:https://issues.apache.org/jira/browse/HBASE-7608[HBASE-7608] for more information about JDK 8
support.

|0.94
|yes
|yes
|N/A
|===

NOTE: 在 HBase 0.98.5 或更早期的版本, 您需要在每个节点的 _hbase-env.sh_ 手动设置 `JAVA_HOME` 。

.操作系统组件
ssh::
  HBase 使用 `ssh` 命令及其相关组件在各节点间通信，因此每个节点都需要能运行 `ssh` 命令并且从master（包括backup的Master）到各节点之间都要能用 `ssh` 通信（包括本地节点）。`ssh` 建议使用共享密钥的模式，而不是密码模式。 Linux/Unix可参考 "<<passwordless.ssh.quickstart>>"， OSX可参考 Hadoop wiki 的 link:http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_%28Single-Node_Cluster%29[SSH: Setting up Remote Desktop and Enabling Self-Login] 。

DNS::
  HBase 使用本地 hostname 来上报IP地址. 0.92.0 及之前的版本需要确保正向/反向 DNS 解析都能生效。 link:https://github.com/sujee/hadoop-dns-checker[hadoop-dns-checker] 可以用来检查集群内的DNS解析是否正常，详情参考其中的`README`。

Loopback IP::
  hbase-0.96.0 之前的版本使用 IP address `127.0.0.1` 来指向 `localhost`, 这个一般不需要显式配置.
  See <<loopback.ip,Loopback IP>> for more details.

NTP::
  集群节点内的时钟必须同步，有一点小偏差（毫秒级等）影响不大，但偏差太大会影响节点之间通信、超时、心跳等众多问题。 因此建议您运行NTP服务（或其他时间同步服务）来确保节点间的时钟同步，这也是安装分布式服务前需要最先检查的重要操作系统配置之一。 安装NTP参考 link:http://www.tldp.org/LDP/sag/html/basic-ntp-config.html[Basic NTP Configuration] at [citetitle]_The Linux Documentation Project (TLDP)_ 。

文件句柄数和进程数限制 (ulimit)::
  Apache HBase 是一个数据库，因此需要短时间内打开较多的文件句柄。您可以在HBase用户下运行 `ulimit -n` 来查看当前用户的句柄数（译者注：一般默认是1024，需要往上调整，大多数情况下调至65536问题不大）. 如果limit太低，您可参见 <<trouble.rs.runtime.filehandles,the Troubleshooting section>> 。 在一些情况下，您可能会遇到如下错误:
+
----
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
----
+
建议句柄数上调至10240。 每个列族至少会拥有 1 个 StoreFile, 在负载下甚至可能多至 6 个 StoreFile。列族和region的数目决定了打开的文件句柄数。 以下是一个RegionServer的文件句柄数的大概计算方式。
+
.潜在文件句柄数计算方式
----
(StoreFiles per ColumnFamily) x (regions per RegionServer)
----
+
举个栗子, 假设 一个 Schema 的每个列族平均有 3 个 StoreFile ，每个 region 有 3 个列族，每个RegionServer大约有100个region，那么 JVM 会打开大约 `3 * 3 * 100 = 900` 个文件句柄（不包含jar、配置等其他的文件打开）。一般来说打开一个文件句柄不会占用太多资源，因此允许一个用户去打开较多的文件句柄相对风险实际并不大。
+
另一个相关的设置是当前用户所允许运行的进程数，Linux/Unix中可使用 `ulimit -u` 设置。注意不要将该命令与 `nproc` 命令混淆， `nproc` 实际控制分配给用户的CPU资源数。如果负载较高时，进程数设置过低可能会引起OOM内存溢出异常。请参考2011年的邮件列表的 `Jack Levin's major HDFS issues`（译者注：对，官网没给链接，自己google吧）。
+
文件句柄数与进程数配置是操作系统级别的配置，需注意配置对象必须是HBase运行的用户。可以查看HBase日志的第一行来确定HBase的启动用户（译者注：事实上更通用的方法是 `ps -ef | grep ...` ）。
+
.`ulimit` Settings on Ubuntu
====
编辑_/etc/security/limits.conf_（一个空格分隔的四列的文件）来在Ubuntu配置ulimit。比如以下示例：第一行设置了hadoop用户的 the number of open files (nofile) 的软限制和硬限制（soft/hard limit）为 32768。 第二行设置了hadoop用户的 the number of processes to 32000。
----
hadoop  -       nofile  32768
hadoop  -       nproc   32000
----
以上配置生效需依赖 Pluggable Authentication Module (PAM) 的配置， 请于 _/etc/pam.d/common-session_ 添加以下内容:
----
session required  pam_limits.so
----
====

Linux Shell::
  有标准的bash就行了 : link:http://www.gnu.org/software/bash[GNU Bash] 。

Windows::
  生产环境不要用Windows。


[[hadoop]]
=== link:http://hadoop.apache.org[Hadoop](((Hadoop)))

The following table summarizes the versions of Hadoop supported with each version of HBase.
下列表格概括了Hadoop版本与HBase版本之间的对应支持关系。基于HBase版本，您需谨慎选择适合的Hadoop版本（包括一些第三方开发版的Hadoop，如HDP、CDH等，See link:http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support[the Hadoop wiki]）。

.基于HBase 1.x ，推荐使用Hadoop 2.x
[TIP]
====
Hadoop 2.x 更快，修复了许多bug，且包含一些新特性，如 short-circuit reads 可以帮助HBase更快地随机读去HDFS文件。
HBase 1.0+ 不再支持 Hadoop 1.x，0.98版本已经开始标注Hadoop 1.1 为 Deprecated。
====


.Hadoop 版本兼容矩阵表

* "S" = supported
* "X" = not supported
* "NT" = Not tested

[cols="1,1,1,1,1,1", options="header"]
|===
| | HBase-0.94.x | HBase-0.98.x (Support for Hadoop 1.1+ is deprecated.) | HBase-1.0.x (Hadoop 1.x is NOT supported) | HBase-1.1.x | HBase-1.2.x
|Hadoop-1.0.x  | X | X | X | X | X
|Hadoop-1.1.x | S | NT | X | X | X
|Hadoop-0.23.x | S | X | X | X | X
|Hadoop-2.0.x-alpha | NT | X | X | X | X
|Hadoop-2.1.0-beta | NT | X | X | X | X
|Hadoop-2.2.0 | NT | S | NT | NT | NT
|Hadoop-2.3.x | NT | S | NT | NT | NT
|Hadoop-2.4.x | NT | S | S | S | S
|Hadoop-2.5.x | NT | S | S | S | S
|Hadoop-2.6.x | NT | NT | S | S | S
|Hadoop-2.7.x | NT | NT | NT | NT | NT
|===

.Replace the Hadoop Bundled With HBase!
[NOTE]
====
Because HBase depends on Hadoop, it bundles an instance of the Hadoop jar under its _lib_ directory.
The bundled jar is ONLY for use in standalone mode.
In distributed mode, it is _critical_ that the version of Hadoop that is out on your cluster match what is under HBase.
Replace the hadoop jar found in the HBase lib directory with the hadoop jar you are running on your cluster to avoid version mismatch issues.
Make sure you replace the jar in HBase everywhere on your cluster.
Hadoop version mismatch issues have various manifestations but often all looks like its hung up.
====

[[hadoop2.hbase_0.94]]
==== Apache HBase 0.94 with Hadoop 2

To get 0.94.x to run on Hadoop 2.2.0, you need to change the hadoop 2 and protobuf versions in the _pom.xml_: Here is a diff with pom.xml changes:

[source]
----
$ svn diff pom.xml
Index: pom.xml
===================================================================
--- pom.xml     (revision 1545157)
+++ pom.xml     (working copy)
@@ -1034,7 +1034,7 @@
     <slf4j.version>1.4.3</slf4j.version>
     <log4j.version>1.2.16</log4j.version>
     <mockito-all.version>1.8.5</mockito-all.version>
-    <protobuf.version>2.4.0a</protobuf.version>
+    <protobuf.version>2.5.0</protobuf.version>
     <stax-api.version>1.0.1</stax-api.version>
     <thrift.version>0.8.0</thrift.version>
     <zookeeper.version>3.4.5</zookeeper.version>
@@ -2241,7 +2241,7 @@
         </property>
       </activation>
       <properties>
-        <hadoop.version>2.0.0-alpha</hadoop.version>
+        <hadoop.version>2.2.0</hadoop.version>
         <slf4j.version>1.6.1</slf4j.version>
       </properties>
       <dependencies>
----

The next step is to regenerate Protobuf files and assuming that the Protobuf has been installed:

* Go to the HBase root folder, using the command line;
* Type the following commands:
+

[source,bourne]
----
$ protoc -Isrc/main/protobuf --java_out=src/main/java src/main/protobuf/hbase.proto
----
+

[source,bourne]
----
$ protoc -Isrc/main/protobuf --java_out=src/main/java src/main/protobuf/ErrorHandling.proto
----


Building against the hadoop 2 profile by running something like the following command:

----
$  mvn clean install assembly:single -Dhadoop.profile=2.0 -DskipTests
----

[[hadoop.hbase_0.94]]
==== Apache HBase 0.92 and 0.94

HBase 0.92 and 0.94 versions can work with Hadoop versions, 0.20.205, 0.22.x, 1.0.x, and 1.1.x.
HBase-0.94 can additionally work with Hadoop-0.23.x and 2.x, but you may have to recompile the code using the specific maven profile (see top level pom.xml)

[[hadoop.hbase_0.96]]
==== Apache HBase 0.96

As of Apache HBase 0.96.x, Apache Hadoop 1.0.x at least is required.
Hadoop 2 is strongly encouraged (faster but also has fixes that help MTTR). We will no longer run properly on older Hadoops such as 0.20.205 or branch-0.20-append.
Do not move to Apache HBase 0.96.x if you cannot upgrade your Hadoop. See link:http://search-hadoop.com/m/7vFVx4EsUb2[HBase, mail # dev - DISCUSS:
                Have hbase require at least hadoop 1.0.0 in hbase 0.96.0?]

[[hadoop.older.versions]]
==== Hadoop versions 0.20.x - 1.x

HBase will lose data unless it is running on an HDFS that has a durable `sync` implementation.
DO NOT use Hadoop 0.20.2, Hadoop 0.20.203.0, and Hadoop 0.20.204.0 which DO NOT have this attribute.
Currently only Hadoop versions 0.20.205.x or any release in excess of this version -- this includes hadoop-1.0.0 -- have a working, durable sync.
The Cloudera blog post link:http://www.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/[An
            update on Apache Hadoop 1.0] by Charles Zedlweski has a nice exposition on how all the Hadoop versions relate.
It's worth checking out if you are having trouble making sense of the Hadoop version morass.

Sync has to be explicitly enabled by setting `dfs.support.append` equal to true on both the client side -- in _hbase-site.xml_ -- and on the serverside in _hdfs-site.xml_ (The sync facility HBase needs is a subset of the append code path).

[source,xml]
----

<property>
  <name>dfs.support.append</name>
  <value>true</value>
</property>
----

You will have to restart your cluster after making this edit.
Ignore the chicken-little comment you'll find in the _hdfs-default.xml_ in the description for the `dfs.support.append` configuration.

[[hadoop.security]]
==== Apache HBase on Secure Hadoop

Apache HBase will run on any Hadoop 0.20.x that incorporates Hadoop security features as long as you do as suggested above and replace the Hadoop jar that ships with HBase with the secure version.
If you want to read more about how to setup Secure HBase, see <<hbase.secure.configuration,hbase.secure.configuration>>.


[[dfs.datanode.max.transfer.threads]]
==== `dfs.datanode.max.transfer.threads` (((dfs.datanode.max.transfer.threads)))

An HDFS DataNode has an upper bound on the number of files that it will serve at any one time.
Before doing any loading, make sure you have configured Hadoop's _conf/hdfs-site.xml_, setting the `dfs.datanode.max.transfer.threads` value to at least the following:

[source,xml]
----

<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>4096</value>
</property>
----

Be sure to restart your HDFS after making the above configuration.

Not having this configuration in place makes for strange-looking failures.
One manifestation is a complaint about missing blocks.
For example:

----
10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
          blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node: java.io.IOException: No live nodes
          contain current block. Will get new block locations from namenode and retry...
----

See also <<casestudies.max.transfer.threads,casestudies.max.transfer.threads>> and note that this property was previously known as `dfs.datanode.max.xcievers` (e.g. link:http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html[Hadoop HDFS: Deceived by Xciever]).

[[zookeeper.requirements]]
=== Zookeeper依赖（ZooKeeper Requirements）

ZooKeeper 3.4.x 是 HBase 1.0.0 的基础组件之一.
HBase makes use of the `multi` functionality that is only available since 3.4.0 (The `useMulti` configuration option defaults to `true` in HBase 1.0.0).
参见背景： link:https://issues.apache.org/jira/browse/HBASE-12241[HBASE-12241 (The crash of regionServer when taking deadserver's replication queue breaks replication)] and link:https://issues.apache.org/jira/browse/HBASE-6775[HBASE-6775 (Use ZK.multi when available for HBASE-6710 0.92/0.94 compatibility fix)] 。

[[standalone_dist]]
== HBase运行模式（HBase run modes: Standalone and Distributed）

HBase has two run modes: <<standalone,standalone>> and <<distributed,distributed>>.
Out of the box, HBase runs in standalone mode.
Whatever your mode, you will need to configure HBase by editing files in the HBase _conf_ directory.
At a minimum, you must edit [code]+conf/hbase-env.sh+ to tell HBase which +java+ to use.
In this file you set HBase environment variables such as the heapsize and other options for the `JVM`, the preferred location for log files, etc.
Set [var]+JAVA_HOME+ to point at the root of your +java+ install.

HBase有两个运行模式: <<standalone,standalone>> & <<distributed,distributed>>。 默认是单机模式，如果要分布式模式你需要编辑 conf 文件夹中的配置文件。

不管是什么模式，你都需要编辑 conf/hbase-env.sh 来告知HBase java的安装路径。在这个文件里你还可以设置HBase的运行环境，诸如 heapsize 和其他 JVM 有关的选项, 还有Log文件地址等。 设置 JAVA_HOME指向 java安装的路径。


[[standalone]]
=== 单机模式（Standalone HBase）

这是默认的模式，在 <<quickstart,quickstart>> 一章中介绍的就是这个模式. 在单机模式中，HBase使用本地文件系统，而不是HDFS ，所有的服务和zooKeeper都运作在一个JVM中。zookeep监听一个端口，这样客户端就可以连接HBase了。

=== Distributed

分布式模式分两种。伪分布式模式是把进程运行在一台机器上，但不是一个JVM；而完全分布式模式就是把整个服务被分布在各个节点上。这种概念来自Hadoop。

Pseudo-distributed mode can run against the local filesystem or it can run against an instance of the _Hadoop Distributed File System_ (HDFS). Fully-distributed mode can ONLY run on HDFS.
HDFS 的搭建教程参考 link:http://hadoop.apache.org/docs/current/[documentation] ，同时可参见一个较完整的guide
  http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide.

[[pseudo]]
==== Pseudo-distributed

.Pseudo-Distributed Quickstart
[NOTE]
====
A quickstart has been added to the <<quickstart,quickstart>> chapter.
See <<quickstart_pseudo,quickstart-pseudo>>.
Some of the information that was originally in this section has been moved there.
====

A pseudo-distributed mode is simply a fully-distributed mode run on a single host.
Use this configuration testing and prototyping on HBase.
Do not use this configuration for production nor for evaluating HBase performance.

[[fully_dist]]
=== Fully-distributed

By default, HBase runs in standalone mode.
Both standalone mode and pseudo-distributed mode are provided for the purposes of small-scale testing.
For a production environment, distributed mode is appropriate.
In distributed mode, multiple instances of HBase daemons run on multiple servers in the cluster.

Just as in pseudo-distributed mode, a fully distributed configuration requires that you set the `hbase-cluster.distributed` property to `true`.
Typically, the `hbase.rootdir` is configured to point to a highly-available HDFS filesystem.

In addition, the cluster is configured so that multiple cluster nodes enlist as RegionServers, ZooKeeper QuorumPeers, and backup HMaster servers.
These configuration basics are all demonstrated in <<quickstart_fully_distributed,quickstart-fully-distributed>>.

.Distributed RegionServers
Typically, your cluster will contain multiple RegionServers all running on different servers, as well as primary and backup Master and Zookeeper daemons.
The _conf/regionservers_ file on the master server contains a list of hosts whose RegionServers are associated with this cluster.
Each host is on a separate line.
All hosts listed in this file will have their RegionServer processes started and stopped when the master server starts or stops.

.ZooKeeper and HBase
See the <<zookeeper,ZooKeeper>> section for ZooKeeper setup instructions for HBase.

.Example Distributed HBase Cluster
====
This is a bare-bones _conf/hbase-site.xml_ for a distributed HBase cluster.
A cluster that is used for real-world work would contain more custom configuration parameters.
Most HBase configuration directives have default values, which are used unless the value is overridden in the _hbase-site.xml_.
See "<<config.files,Configuration Files>>" for more information.

[source,xml]
----

<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://namenode.example.org:8020/hbase</value>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>node-a.example.com,node-b.example.com,node-c.example.com</value>
  </property>
</configuration>
----

This is an example _conf/regionservers_ file, which contains a list of nodes that should run a RegionServer in the cluster.
These nodes need HBase installed and they need to use the same contents of the _conf/_ directory as the Master server

[source]
----

node-a.example.com
node-b.example.com
node-c.example.com
----

This is an example _conf/backup-masters_ file, which contains a list of each node that should run a backup Master instance.
The backup Master instances will sit idle unless the main Master becomes unavailable.

[source]
----

node-b.example.com
node-c.example.com
----
====

.Distributed HBase Quickstart
See <<quickstart_fully_distributed,quickstart-fully-distributed>> for a walk-through of a simple three-node cluster configuration with multiple ZooKeeper, backup HMaster, and RegionServer instances.

.Procedure: HDFS Client Configuration
. Of note, if you have made HDFS client configuration changes on your Hadoop cluster, such as configuration directives for HDFS clients, as opposed to server-side configurations, you must use one of the following methods to enable HBase to see and use these configuration changes:
+
a. Add a pointer to your `HADOOP_CONF_DIR` to the `HBASE_CLASSPATH` environment variable in _hbase-env.sh_.
b. Add a copy of _hdfs-site.xml_ (or _hadoop-site.xml_) or, better, symlinks, under _${HBASE_HOME}/conf_, or
c. if only a small set of HDFS client configurations, add them to _hbase-site.xml_.


An example of such an HDFS client configuration is `dfs.replication`.
If for example, you want to run with a replication factor of 5, HBase will create files with the default of 3 unless you do the above to make the configuration available to HBase.

[[confirm]]
== Running and Confirming Your Installation

首先确认你的HDFS是运行着的。你可以运行HADOOP_HOME中的 bin/start-hdfs.sh 来启动HDFS。你可以通过put命令来测试放一个文件，然后有get命令来读这个文件。通常情况下HBase是不会运行mapreduce的，所以不需要检查MR。

如果你自己管理ZooKeeper集群，你需要确认它是运行着的。如果是HBase托管，ZoopKeeper 会随HBase启动。

用如下命令启动HBase:

----
bin/start-hbase.sh
----

这个脚本在HBASE_HOME目录里面。
你现在已经启动HBase了。HBase把log记在 logs 子目录里面. 当HBase启动出问题的时候，可以看看Log.

HBase也有一个UI界面，上面会列出重要的属性。默认是在Master的16010端口上(HBase RegionServers 会默认绑定 16020端口，在端口16030上有一个展示信息的HTTP界面 )。如果Master运行在 master.example.org，端口是默认的话，你可以用浏览器在 http://master.example.org:16010看到主界面. .

0.98 之前的版本默认端口不是16xxx，而是60xxx，比如60010为UI端口。

HBase启动后，参见 <<shell_exercises,shell exercises>> 可以看到如何建表，插入数据，scan你的表，还有disable这个表，最后把它删掉。

退出shell后可以尝试停止HBase。

----
$ ./bin/stop-hbase.sh
stopping hbase...............
----

停止操作需要一些时间，你的集群越大，停的时间可能会越长。如果你正在运行一个分布式的操作，要确认在HBase彻底停止之前，Hadoop不能停.



[[config.files]]
== 默认配置（Default Configuration）

[[hbase.site]]
=== _hbase-site.xml_ and _hbase-default.xml_

正如Hadoop放置HDFS的配置文件hdfs-site.xml，HBase的配置文件是 conf/hbase-site.xml. 你可以在 <<hbase_default_configurations,hbase default configurations>> 找到配置的属性列表。你也可以在src/main/resources目录下找到hbase-default.xml。

不是所有的配置都在 hbase-default.xml出现。只要改了代码，配置就有可能改变，所以唯一了解这些被改过的配置的办法是读源代码本身。

要注意的是，要重启集群才能使配置生效。

// hbase/src/main/asciidoc
//
include::../../../../target/asciidoc/hbase-default.adoc[]


[[hbase.env.sh]]
=== _hbase-env.sh_

在这个文件里面设置HBase环境变量。比如可以配置JVM启动的堆大小或者GC的参数。你还可在这里配置HBase的参数，如Log位置，niceness(译者注:优先级)，ssh参数还有pid文件的位置等等。打开文件conf/hbase-env.sh细读其中的内容。每个选项都是有详尽的注释的。你可以在此添加自己的环境变量。

这个文件的改动只有HBase重启才能生效。

[[log4j]]
=== _log4j.properties_

编辑这个文件可以改变HBase的日志的级别，轮滚策略等等。

通过HBase UI可以修改log4j配置，但实际上这个文件的改动只有HBase重启才能生效。

[[client_dependencies]]
=== 连接HBase的客户端配置与依赖（Client configuration and dependencies connecting to an HBase cluster）

If you are running HBase in standalone mode, you don't need to configure anything for your client to work provided that they are all on the same machine.

Since the HBase Master may move around, clients bootstrap by looking to ZooKeeper for current critical locations.
ZooKeeper is where all these values are kept.
Thus clients require the location of the ZooKeeper ensemble before they can do anything else.
Usually this the ensemble location is kept out in the _hbase-site.xml_ and is picked up by the client from the `CLASSPATH`.

If you are configuring an IDE to run a HBase client, you should include the _conf/_ directory on your classpath so _hbase-site.xml_ settings can be found (or add _src/test/resources_ to pick up the hbase-site.xml used by tests).

HBase客户端的classpath至少需要包括如下的jar:
[source]
----

commons-configuration (commons-configuration-1.6.jar)
commons-lang (commons-lang-2.5.jar)
commons-logging (commons-logging-1.1.1.jar)
hadoop-core (hadoop-core-1.0.0.jar)
hbase (hbase-0.92.0.jar)
log4j (log4j-1.2.16.jar)
slf4j-api (slf4j-api-1.5.8.jar)
slf4j-log4j (slf4j-log4j12-1.5.8.jar)
zookeeper (zookeeper-3.4.2.jar)
----

一个基本的客户端示例 _hbase-site.xml_ 可能如下:
[source,xml]
----
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>example1,example2,example3</value>
    <description>The directory shared by region servers.
    </description>
  </property>
</configuration>
----

[[java.client.config]]
==== Java客户端配置（Java client configuration）

Java客户端的配置存放在实例 link:http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HBaseConfiguration[HBaseConfiguration] 中。

HBaseConfiguration有一个工厂方法, HBaseConfiguration.create();,运行这个方法的时候，他会去CLASSPATH 下找hbase-site.xml，读他发现的第一个配置文件的内容。 (这个方法还会去找hbase-default.xml ; hbase.X.X.X.jar里面也会有一个an hbase-default.xml). 不使用任何hbase-site.xml文件直接通过Java代码注入配置信息也是可以的。例如，你可以用编程的方式设置ZooKeeper信息，只要这样做:

[source,java]
----
Configuration config = HBaseConfiguration.create();
config.set("hbase.zookeeper.quorum", "localhost");  // Here we are running zookeeper locally
----

如果有多ZooKeeper实例，你可以使用逗号列表。(就像在hbase-site.xml 文件中做得一样). 这个 Configuration 实例会被传递到 link:http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Table.html[Table], 之类的实例里面去.

[[example_config]]
== Example Configurations

=== Basic Distributed HBase Install

Here is an example basic configuration for a distributed ten node cluster:
* The nodes are named `example0`, `example1`, etc., through node `example9` in this example.
* The HBase Master and the HDFS NameNode are running on the node `example0`.
* RegionServers run on nodes `example1`-`example9`.
* A 3-node ZooKeeper ensemble runs on `example1`, `example2`, and `example3` on the default ports.
* ZooKeeper data is persisted to the directory _/export/zookeeper_.

Below we show what the main configuration files -- _hbase-site.xml_, _regionservers_, and _hbase-env.sh_ -- found in the HBase _conf_ directory might look like.

[[hbase_site]]
==== _hbase-site.xml_

[source,xml]
----
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>example1,example2,example3</value>
    <description>The directory shared by RegionServers.
    </description>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/export/zookeeper</value>
    <description>Property from ZooKeeper config zoo.cfg.
    The directory where the snapshot is stored.
    </description>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://example0:8020/hbase</value>
    <description>The directory shared by RegionServers.
    </description>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
    <description>The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    </description>
  </property>
</configuration>
----

[[regionservers]]
==== _regionservers_

In this file you list the nodes that will run RegionServers.
In our case, these nodes are `example1`-`example9`.

[source]
----
example1
example2
example3
example4
example5
example6
example7
example8
example9
----

[[hbase_env]]
==== _hbase-env.sh_

The following lines in the _hbase-env.sh_ file show how to set the `JAVA_HOME` environment variable (required for HBase 0.98.5 and newer) and set the heap to 4 GB (rather than the default value of 1 GB). If you copy and paste this example, be sure to adjust the `JAVA_HOME` to suit your environment.

----
# The java implementation to use.
export JAVA_HOME=/usr/java/jdk1.7.0/

# The maximum amount of heap to use. Default is left to JVM default.
export HBASE_HEAPSIZE=4G
----

Use +rsync+ to copy the content of the _conf_ directory to all nodes of the cluster.

[[important_configurations]]
== The Important Configurations

Below we list some _important_ configurations.
We've divided this section into required configuration and worth-a-look recommended configs.

[[required_configuration]]
=== Required Configurations

Review the <<os,os>> and <<hadoop,hadoop>> sections.

[[big.cluster.config]]
==== Big Cluster Configurations

If you have a cluster with a lot of regions, it is possible that a Regionserver checks in briefly after the Master starts while all the remaining RegionServers lag behind. This first server to check in will be assigned all regions which is not optimal.
To prevent the above scenario from happening, up the `hbase.master.wait.on.regionservers.mintostart` property from its default value of 1.
See link:https://issues.apache.org/jira/browse/HBASE-6389[HBASE-6389 Modify the
            conditions to ensure that Master waits for sufficient number of Region Servers before
            starting region assignments] for more detail.

[[backup.master.fail.fast]]
==== If a backup Master exists, make the primary Master fail fast

If the primary Master loses its connection with ZooKeeper, it will fall into a loop where it keeps trying to reconnect.
Disable this functionality if you are running more than one Master: i.e. a backup Master.
Failing to do so, the dying Master may continue to receive RPCs though another Master has assumed the role of primary.
See the configuration <<fail.fast.expired.active.master,fail.fast.expired.active.master>>.

=== 推荐配置（Recommended Configurations）

[[recommended_configurations.zk]]
==== Zookeeper配置（ZooKeeper Configuration）

[[sect.zookeeper.session.timeout]]
===== `zookeeper.session.timeout`

这个默认值是3分钟。这意味着一旦一个server宕掉了，Master至少需要3分钟才能察觉到宕机，开始恢复。你可能希望将这个超时调短，这样Master就能更快的察觉到了。在你调这个值之前，你需要确认你的JVM的GC参数，否则一个长时间的GC操作就可能导致超时。（当一个RegionServer在运行一个长时间的GC的时候，你可能想要重启并恢复它）.

要想改变这个配置，可以编辑 hbase-site.xml, 将配置部署到全部集群，然后重启。

我们之所以把这个值调的很高，是因为我们不想一天到晚在论坛里回答新手的问题。“为什么我在执行一个大规模数据导入的时候Region Server死掉啦”，通常这样的问题是因为长时间的GC操作引起的，他们的JVM没有调优。我们是这样想的，如果一个人对HBase不很熟悉，不能期望他知道所有的问题。等到他逐渐熟悉了，他就可以自己调这个参数了。

[[zookeeper.instances]]
===== Zookeeper实例数目（Number of ZooKeeper Instances）

参见 <<zookeeper,zookeeper>>.

[[recommended.configurations.hdfs]]
==== HDFS配置（HDFS Configurations）

[[dfs.datanode.failed.volumes.tolerated]]
===== dfs.datanode.failed.volumes.tolerated

This is the "...number of volumes that are allowed to fail before a DataNode stops offering service.
By default any volume failure will cause a datanode to shutdown" from the _hdfs-default.xml_ description.
You might want to set this to about half the amount of your available disks.

[[hbase.regionserver.handler.count_description]]
==== `hbase.regionserver.handler.count`

这个设置决定了处理用户请求的线程数量，默认是10。这个值设的比较小，主要是为了预防用户用一个比较大的写缓冲，同时如果还有较多客户端并发，这样region servers会垮掉。有经验的做法是，当请求内容很大(上MB，如大puts, 使用缓存的scans)的时候，把这个值调低。请求内容较小的时候(gets, 小puts, ICVs, deletes)，把这个值调大。（译者注：其实就是确保总的请求内存不要太大，粗略评估就是 handler-count * mem-per-request）

当客户端的单个请求内容很小的时候，把这个值设置的和最大客户端数量一致是可接受的，甚至可以提升性能。一个典型的例子就是一个给网站服务的集群，put操作一般不会缓冲，绝大多数的操作是get操作。

把这个值放大的危险之处在于，把所有的Put操作缓冲意味着对内存有很大的压力，甚至会导致OutOfMemory。一个内存不够大的RegionServer会频繁的触发GC操作，渐渐用户就能感受到停顿。(因为所有请求内容所占用的内存不管GC执行几遍也是不能回收的)。一段时间后，集群也会受到影响，因为所有的指向这个region的请求都会变慢。这样就会拖累集群，加剧了这个问题。

你可以根据 <<rpc.logging,rpc.logging>> 在单个RegionServer启动log并查看log末尾 (请求队列消耗内存)，去评估自己的该配置是否过大或过小。

[[big_memory]]
==== 大内存机器的配置（Configuration for large memory machines）

HBase有一个合理的保守的配置，这样可以运作在所有的机器上。如果你有台大内存的集群-HBase有8G或者更大的heap，接下来的配置可能会帮助你。 TODO. (译者注：TODO是官方这么写的，实际的大多数线上集群应该在20~50g左右，可以参考许多大公司分享的内存配置案例，包括GC配置等)

[[config.compression]]
==== 压缩（Compression）

应该考虑启用 ColumnFamily 压缩。有好几个选项，通过降低存储文件大小以降低IO，降低消耗且大多情况下提高性能。

参见 <<compression,compression>>

[[config.wals]]
==== Configuring the size and number of WAL files

HBase uses <<wal,wal>> to recover the memstore data that has not been flushed to disk in case of an RS failure.
These WAL files should be configured to be slightly smaller than HDFS block (by default a HDFS block is 64Mb and a WAL file is ~60Mb).

HBase also has a limit on the number of WAL files, designed to ensure there's never too much data that needs to be replayed during recovery.
This limit needs to be set according to memstore configuration, so that all the necessary data would fit.
It is recommended to allocate enough WAL files to store at least that much data (when all memstores are close to full). For example, with 16Gb RS heap, default memstore settings (0.4), and default WAL file size (~60Mb), 16Gb*0.4/60, the starting point for WAL file count is ~109.
However, as all memstores are not expected to be full all the time, less WAL files can be allocated.

[[disable.splitting]]
==== Splitting管理（Managed Splitting）

HBase generally handles splitting your regions, based upon the settings in your _hbase-default.xml_ and _hbase-site.xml_          configuration files.
Important settings include `hbase.regionserver.region.split.policy`, `hbase.hregion.max.filesize`, `hbase.regionserver.regionSplitLimit`.
A simplistic view of splitting is that when a region grows to `hbase.hregion.max.filesize`, it is split.
For most use patterns, most of the time, you should use automatic splitting.
See <<manual_region_splitting_decisions,manual region splitting decisions>> for more information about manual region splitting.

Instead of allowing HBase to split your regions automatically, you can choose to manage the splitting yourself.
This feature was added in HBase 0.90.0.
Manually managing splits works if you know your keyspace well, otherwise let HBase figure where to split for you.
Manual splitting can mitigate region creation and movement under load.
It also makes it so region boundaries are known and invariant (if you disable region splitting). If you use manual splits, it is easier doing staggered, time-based major compactions to spread out your network IO load.

.Disable Automatic Splitting
To disable automatic splitting, set `hbase.hregion.max.filesize` to a very large value, such as `100 GB` It is not recommended to set it to its absolute maximum value of `Long.MAX_VALUE`.

.Automatic Splitting Is Recommended
[NOTE]
====
If you disable automatic splits to diagnose a problem or during a period of fast data growth, it is recommended to re-enable them when your situation becomes more stable.
The potential benefits of managing region splits yourself are not undisputed.
====

.Determine the Optimal Number of Pre-Split Regions
The optimal number of pre-split regions depends on your application and environment.
A good rule of thumb is to start with 10 pre-split regions per server and watch as data grows over time.
It is better to err on the side of too few regions and perform rolling splits later.
The optimal number of regions depends upon the largest StoreFile in your region.
The size of the largest StoreFile will increase with time if the amount of data grows.
The goal is for the largest region to be just large enough that the compaction selection algorithm only compacts it during a timed major compaction.
Otherwise, the cluster can be prone to compaction storms where a large number of regions under compaction at the same time.
It is important to understand that the data growth causes compaction storms, and not the manual split decision.

If the regions are split into too many large regions, you can increase the major compaction interval by configuring `HConstants.MAJOR_COMPACTION_PERIOD`.
HBase 0.90 introduced `org.apache.hadoop.hbase.util.RegionSplitter`, which provides a network-IO-safe rolling split of all regions.

[[managed.compactions]]
==== Compactions管理（Managed Compactions）

默认 major compaction 周期是七天一次，0.96.x之前的版本是默认一天一次。如果需要精确控制 major compaction 的时间和周期，可以禁止周期性 major compaction。
细节参见 <<compaction.parameters,compaction.parameters>> 的 `hbase.hregion.majorcompaction`。
（译者注：major compaction 会比较影响性能，一些对性能要求高的HBase集群会禁止周期性自动的 major compaction 而改为手动在闲时触发）


.请勿完全禁止 major compaction
[WARNING]
====
Major compactions 是清理归并 StoreFile 必须的操作，因此请勿完全禁止。
您可以在hbase shell 或通过 HBaseAdmin API 去手动触发。 http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Admin.html#majorCompact(org.apache.hadoop.hbase.TableName)[Admin API]
====

更多 compaction 细节参见 <<compaction,compaction>>

[[spec.ex]]
==== 推测执行（Speculative Execution）

MapReduce任务的预测执行缺省是打开的，HBase集群一般建议在系统级关闭预测执行，除非在某种特殊情况下需要打开，此时可以每任务配置。设置mapred.map.tasks.speculative.execution 和 mapred.reduce.tasks.speculative.execution 为 false。

[[other_configuration]]
=== Other Configurations

[[balancer_config]]
==== 负载均衡（Balancer）

负载均衡器(LoadBalancer)是在主服务器上运行的定期操作，以重新分布集群区域。通过hbase.balancer.period 设置，缺省值300000 (5 分钟).

参见 <<master.processes.loadbalancer,master.processes.loadbalancer>> 获取更多细节关于 LoadBalancer.

[[disabling.blockcache]]
==== 禁止块缓存（Disabling Blockcache）

不要关闭块缓存 (通过hbase.block.cache.size 为 0 来设置)。当前如果关闭块缓存会很不好，因为区域服务器会花很多时间不停加载hfile指数。如果工作集如此配置块缓存没有好处，最少应保证hfile指数保存在块缓存内的大小(可以通过查询区域服务器的UI，得到大致的数值。可以看到网页的上方有块指数值统计)。

[[nagles]]
==== link:http://en.wikipedia.org/wiki/Nagle's_algorithm[Nagle's] 或小包问题

如果操作 HBase 时看到大量40ms左右的偶然延时，尝试Nagles配置。如，参考用户邮件列表 link:http://search-hadoop.com/m/pduLg2fydtE/Inconsistent+scan+performance+with+caching+set+&subj=Re+Inconsistent+scan+performance+with+caching+set+to+1[Inconsistent scan performance with caching set to 1] ，该 issue 在其中启用tcpNoDelay (译者注，本英文原文notcpdelay有误) 提高了扫描速度。你也可以查看该文档的尾部图表：link:https://issues.apache.org/jira/browse/HBASE-7008[HBASE-7008 Set scanner caching to a better default]，Lars Hofhansl 尝试了各种不同的数据大小，Nagle打开或关闭的测量结果。

[[mttr]]
==== Better Mean Time to Recover (MTTR)

This section is about configurations that will make servers come back faster after a fail.
See the Deveraj Das an Nicolas Liochon blog post link:http://hortonworks.com/blog/introduction-to-hbase-mean-time-to-recover-mttr/[Introduction to HBase Mean Time to Recover (MTTR)] for a brief introduction.

The issue link:https://issues.apache.org/jira/browse/HBASE-8389[HBASE-8354 forces Namenode into loop with lease recovery requests] is messy but has a bunch of good discussion toward the end on low timeouts and how to effect faster recovery including citation of fixes added to HDFS. Read the Varun Sharma comments.
The below suggested configurations are Varun's suggestions distilled and tested.
Make sure you are running on a late-version HDFS so you have the fixes he refers too and himself adds to HDFS that help HBase MTTR (e.g.
HDFS-3703, HDFS-3712, and HDFS-4791 -- Hadoop 2 for sure has them and late Hadoop 1 has some). Set the following in the RegionServer.

[source,xml]
----
<property>
  <name>hbase.lease.recovery.dfs.timeout</name>
  <value>23000</value>
  <description>How much time we allow elapse between calls to recover lease.
  Should be larger than the dfs timeout.</description>
</property>
<property>
  <name>dfs.client.socket-timeout</name>
  <value>10000</value>
  <description>Down the DFS timeout from 60 to 10 seconds.</description>
</property>
----

And on the NameNode/DataNode side, set the following to enable 'staleness' introduced in HDFS-3703, HDFS-3912.

[source,xml]
----
<property>
  <name>dfs.client.socket-timeout</name>
  <value>10000</value>
  <description>Down the DFS timeout from 60 to 10 seconds.</description>
</property>
<property>
  <name>dfs.datanode.socket.write.timeout</name>
  <value>10000</value>
  <description>Down the DFS timeout from 8 * 60 to 10 seconds.</description>
</property>
<property>
  <name>ipc.client.connect.timeout</name>
  <value>3000</value>
  <description>Down from 60 seconds to 3.</description>
</property>
<property>
  <name>ipc.client.connect.max.retries.on.timeouts</name>
  <value>2</value>
  <description>Down from 45 seconds to 3 (2 == 3 retries).</description>
</property>
<property>
  <name>dfs.namenode.avoid.read.stale.datanode</name>
  <value>true</value>
  <description>Enable stale state in hdfs</description>
</property>
<property>
  <name>dfs.namenode.stale.datanode.interval</name>
  <value>20000</value>
  <description>Down from default 30 seconds</description>
</property>
<property>
  <name>dfs.namenode.avoid.write.stale.datanode</name>
  <value>true</value>
  <description>Enable stale state in hdfs</description>
</property>
----

[[jmx_config]]
==== JMX

JMX (Java Management Extensions) provides built-in instrumentation that enables you to monitor and manage the Java VM.
To enable monitoring and management from remote systems, you need to set system property `com.sun.management.jmxremote.port` (the port number through which you want to enable JMX RMI connections) when you start the Java VM.
See the link:http://docs.oracle.com/javase/6/docs/technotes/guides/management/agent.html[official documentation] for more information.
Historically, besides above port mentioned, JMX opens two additional random TCP listening ports, which could lead to port conflict problem. (See link:https://issues.apache.org/jira/browse/HBASE-10289[HBASE-10289] for details)

As an alternative, You can use the coprocessor-based JMX implementation provided by HBase.
To enable it in 0.99 or above, add below property in _hbase-site.xml_:

[source,xml]
----
<property>
  <name>hbase.coprocessor.regionserver.classes</name>
  <value>org.apache.hadoop.hbase.JMXListener</value>
</property>
----

NOTE: DO NOT set `com.sun.management.jmxremote.port` for Java VM at the same time.

Currently it supports Master and RegionServer Java VM.
By default, the JMX listens on TCP port 10102, you can further configure the port using below properties:

[source,xml]
----
<property>
  <name>regionserver.rmi.registry.port</name>
  <value>61130</value>
</property>
<property>
  <name>regionserver.rmi.connector.port</name>
  <value>61140</value>
</property>
----

The registry port can be shared with connector port in most cases, so you only need to configure regionserver.rmi.registry.port.
However if you want to use SSL communication, the 2 ports must be configured to different values.

By default the password authentication and SSL communication is disabled.
To enable password authentication, you need to update _hbase-env.sh_          like below:
[source,bash]
----
export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.authenticate=true                  \
                       -Dcom.sun.management.jmxremote.password.file=your_password_file   \
                       -Dcom.sun.management.jmxremote.access.file=your_access_file"

export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
----

See example password/access file under _$JRE_HOME/lib/management_.

To enable SSL communication with password authentication, follow below steps:

[source,bash]
----
#1. generate a key pair, stored in myKeyStore
keytool -genkey -alias jconsole -keystore myKeyStore

#2. export it to file jconsole.cert
keytool -export -alias jconsole -keystore myKeyStore -file jconsole.cert

#3. copy jconsole.cert to jconsole client machine, import it to jconsoleKeyStore
keytool -import -alias jconsole -keystore jconsoleKeyStore -file jconsole.cert
----

And then update _hbase-env.sh_ like below:

[source,bash]
----
export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=true                         \
                       -Djavax.net.ssl.keyStore=/home/tianq/myKeyStore                 \
                       -Djavax.net.ssl.keyStorePassword=your_password_in_step_1       \
                       -Dcom.sun.management.jmxremote.authenticate=true                \
                       -Dcom.sun.management.jmxremote.password.file=your_password file \
                       -Dcom.sun.management.jmxremote.access.file=your_access_file"

export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE "
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE "
----

最后客户端侧启动 `jconsole` on the client using the key store:

[source,bash]
----
jconsole -J-Djavax.net.ssl.trustStore=/home/tianq/jconsoleKeyStore
----

NOTE: 要启用Master的 HBase JMX 实现, 以下配置需添加到 _hbase-site.xml_:

[source,xml]
----
<property>
  <ame>hbase.coprocessor.master.classes</name>
  <value>org.apache.hadoop.hbase.JMXListener</value>
</property>
----

相关的端口配置项为 `master.rmi.registry.port` (by default 10101) and `master.rmi.connector.port` (by default the same as registry.port)

[[dyn_config]]
== 动态配置（Dynamic Configuration）

从 HBase 1.0.0 起, 一部分配置的改变与生效可不需要重启服务，我们称其为动态配置.
您可在 hbase shell 中执行 `update_config` 和 `update_all_config` 来更新当前节点的server或所有节点的server去重新加载动态配置。

以下为部分的动态配置示例: `hbase.regionserver.thread.compaction.large`, `hbase.regionserver.thread.compaction.small`, `hbase.regionserver.thread.split`, `hbase.regionserver.thread.merge`, compaction的policy、configuration和触发时间的调整等。
较为完整的动态配置列表可参见  link:https://issues.apache.org/jira/browse/HBASE-12147[HBASE-12147 Porting Online Config Change from 89-fb].

ifdef::backend-docbook[]
[index]
== Index
// Generated automatically by the DocBook toolchain.
endif::backend-docbook[]
